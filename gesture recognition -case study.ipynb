{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize as imresize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('C:\\cnndatasets\\Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('C:\\cnndatasets\\Project_data/val.csv').readlines())\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 160 # image width\n",
    "z = 160 # image height\n",
    "channels=3\n",
    "classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[x for x in range(0,x)]   #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size    # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = \"C:\\cnndatasets\\Project_data/train\"\n",
    "val_path = \"C:\\cnndatasets\\Project_data/val\"\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs =   10  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_batches = num_train_sequences//batch_size \n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Creating a model with 160x160 image size,epochs=10 and batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "#write your model here\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Conv3D(8, #number of filters \n",
    "                 kernel_size=(3,3,3), \n",
    "                 input_shape=(x,y,z,channels),\n",
    "                 padding='same'))\n",
    "\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_1.add(Conv3D(16, #Number of filters, \n",
    "                 kernel_size=(3,3,3), \n",
    "                 padding='same'))\n",
    "\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model_1.add(Conv3D(32, #Number of filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "model_1.add(Conv3D(64, #Number pf filters \n",
    "                 kernel_size=(1,3,3), \n",
    "                 padding='same'))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model_1.add(Flatten())\n",
    "\n",
    "model_1.add(Dense(100, activation='relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "#softmax layer\n",
    "model_1.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 30, 160, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 160, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 160, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 15, 80, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 15, 80, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 80, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 7, 40, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 7, 40, 40, 32)     4640      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 40, 40, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 20, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 20, 20, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 20, 20, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               7680100   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 7,708,349\n",
      "Trainable params: 7,708,109\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model_1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 32\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,8,30,25600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/batch_normalization/FusedBatchNormV3 (defined at <ipython-input-12-16518857c690>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1425]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-16518857c690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,8,30,25600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node sequential/batch_normalization/FusedBatchNormV3 (defined at <ipython-input-12-16518857c690>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1425]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model_1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We had hit the limit on memory resources with image resolution of 160x160 with 30 frames and batch_size of 32...we get the below error\n",
    "\n",
    "ResourceExhaustedError: OOM when allocating tensor with shape[32,8,30,25600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp -2 :   Reduce the batch size to 20 and image dimensions to 60x60 with 30 frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=3\n",
    "classes=5\n",
    "\n",
    "\n",
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "batch_size=20\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[x for x in range(0,x)]   #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size    # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = \"C:\\cnndatasets\\Project_data/train\"\n",
    "val_path = \"C:\\cnndatasets\\Project_data/val\"\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs =   10  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_batches = num_train_sequences//batch_size \n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "model_b = Sequential()\n",
    "model_b.add(Conv3D(16, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(BatchNormalization())\n",
    "model_b.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_b.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(BatchNormalization())\n",
    "model_b.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_b.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_b.add(Activation('relu'))\n",
    "model_b.add(BatchNormalization())\n",
    "model_b.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_b.add(Flatten())\n",
    "model_b.add(Dense(128, activation='relu'))\n",
    "model_b.add(Dropout(0.25))\n",
    "model_b.add(Dense(64, activation='relu'))\n",
    "model_b.add(Dropout(0.25))\n",
    "model_b.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 30, 60, 60, 16)    1312      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 15, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 15, 30, 30, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 15, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 7, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 7, 15, 15, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 7, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 3, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9408)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               1204352   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,283,909\n",
      "Trainable params: 1,283,685\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model_b.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_b.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
    "callbacks_list = [checkpoint, LR,earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 20\n",
      "Epoch 1/10\n",
      "32/34 [===========================>..] - ETA: 13s - loss: 2.4085 - categorical_accuracy: 0.2828Batch:  34 Index: 20\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.3875 - categorical_accuracy: 0.2821Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 252s 8s/step - loss: 2.3875 - categorical_accuracy: 0.2821 - val_loss: 1.8156 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2407_52_42.683254\\model-00001-2.38754-0.28205-1.81556-0.23000.h5\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 41s 1s/step - loss: 2.3757 - categorical_accuracy: 0.2745 - val_loss: 2.5497 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2407_52_42.683254\\model-00002-2.37569-0.27451-2.54966-0.21000.h5\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 42s 1s/step - loss: 2.2074 - categorical_accuracy: 0.2549 - val_loss: 4.3984 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2407_52_42.683254\\model-00003-2.20736-0.25490-4.39838-0.20000.h5\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 44s 1s/step - loss: 2.5021 - categorical_accuracy: 0.2157 - val_loss: 3.9155 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2407_52_42.683254\\model-00004-2.50209-0.21569-3.91546-0.17000.h5\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 43s 1s/step - loss: 2.5477 - categorical_accuracy: 0.1667 - val_loss: 3.2277 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2407_52_42.683254\\model-00005-2.54770-0.16667-3.22769-0.21000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 45s 1s/step - loss: 2.0095 - categorical_accuracy: 0.2353 - val_loss: 3.1839 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2407_52_42.683254\\model-00006-2.00954-0.23529-3.18391-0.18000.h5\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 54s 2s/step - loss: 1.6471 - categorical_accuracy: 0.3235 - val_loss: 3.3044 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2407_52_42.683254\\model-00007-1.64712-0.32353-3.30442-0.23000.h5\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 52s 2s/step - loss: 1.6042 - categorical_accuracy: 0.2255 - val_loss: 3.3378 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2407_52_42.683254\\model-00008-1.60422-0.22549-3.33779-0.20000.h5\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.6656 - categorical_accuracy: 0.2941 - val_loss: 3.1220 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2407_52_42.683254\\model-00009-1.66559-0.29412-3.12199-0.21000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.4679 - categorical_accuracy: 0.2941 - val_loss: 3.6991 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2407_52_42.683254\\model-00010-1.46795-0.29412-3.69911-0.16000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4ea3762b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-3 :adding more layers and increase the no of epochs to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 60 # image width\n",
    "z = 60 # image height\n",
    "\n",
    "batch_size=20\n",
    "num_epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b1 = Sequential()\n",
    "model_b1.add(Conv3D(16, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_b1.add(Activation('relu'))\n",
    "model_b1.add(BatchNormalization())\n",
    "\n",
    "model_b1.add(Conv3D(16, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_b1.add(Activation('relu'))\n",
    "model_b1.add(BatchNormalization())\n",
    "\n",
    "model_b1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_b1.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_b1.add(Activation('relu'))\n",
    "model_b1.add(BatchNormalization())\n",
    "\n",
    "model_b1.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_b1.add(Activation('relu'))\n",
    "model_b1.add(BatchNormalization())\n",
    "\n",
    "model_b1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_b1.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_b1.add(Activation('relu'))\n",
    "model_b1.add(BatchNormalization())\n",
    "\n",
    "model_b1.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_b1.add(Activation('relu'))\n",
    "model_b1.add(BatchNormalization())\n",
    "\n",
    "model_b1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_b1.add(Flatten())\n",
    "model_b1.add(Dense(128, activation='relu'))\n",
    "model_b1.add(Dropout(0.25))\n",
    "model_b1.add(Dense(64, activation='relu'))\n",
    "model_b1.add(Dropout(0.25))\n",
    "model_b1.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_7 (Conv3D)            (None, 30, 60, 60, 16)    1312      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 30, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 30, 60, 60, 16)    6928      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 30, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 15, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 15, 30, 30, 32)    13856     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 15, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 15, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 15, 30, 30, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 15, 30, 30, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 15, 30, 30, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 7, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 7, 15, 15, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 7, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 7, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 7, 15, 15, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 7, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 7, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 3, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9408)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               1204352   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,429,621\n",
      "Trainable params: 1,429,173\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model_b1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_b1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 20\n",
      "Epoch 1/15\n",
      "32/34 [===========================>..] - ETA: 10s - loss: 2.8222 - categorical_accuracy: 0.3125Batch:  34 Index: 20\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.7840 - categorical_accuracy: 0.3122Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 221s 7s/step - loss: 2.7840 - categorical_accuracy: 0.3122 - val_loss: 2.5126 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2407_52_42.683254\\model-00001-2.78396-0.31222-2.51257-0.16000.h5\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - 39s 1s/step - loss: 2.5264 - categorical_accuracy: 0.2745 - val_loss: 2.6799 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2407_52_42.683254\\model-00002-2.52636-0.27451-2.67990-0.23000.h5\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - 40s 1s/step - loss: 2.1670 - categorical_accuracy: 0.3529 - val_loss: 2.9849 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2407_52_42.683254\\model-00003-2.16703-0.35294-2.98488-0.18000.h5\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - 37s 1s/step - loss: 2.5789 - categorical_accuracy: 0.2451 - val_loss: 6.4064 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2407_52_42.683254\\model-00004-2.57890-0.24510-6.40638-0.16000.h5\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - 37s 1s/step - loss: 2.8137 - categorical_accuracy: 0.2353 - val_loss: 2.4372 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2407_52_42.683254\\model-00005-2.81370-0.23529-2.43718-0.24000.h5\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - 41s 1s/step - loss: 2.0141 - categorical_accuracy: 0.2549 - val_loss: 1.9612 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2407_52_42.683254\\model-00006-2.01413-0.25490-1.96117-0.21000.h5\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - 39s 1s/step - loss: 2.1066 - categorical_accuracy: 0.2059 - val_loss: 2.1000 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2407_52_42.683254\\model-00007-2.10662-0.20588-2.10004-0.25000.h5\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - 39s 1s/step - loss: 2.3269 - categorical_accuracy: 0.2843 - val_loss: 11.5800 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2407_52_42.683254\\model-00008-2.32687-0.28431-11.58000-0.22000.h5\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - 39s 1s/step - loss: 3.4811 - categorical_accuracy: 0.3333 - val_loss: 6.5549 - val_categorical_accuracy: 0.1300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2407_52_42.683254\\model-00009-3.48110-0.33333-6.55491-0.13000.h5\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - 37s 1s/step - loss: 2.4068 - categorical_accuracy: 0.2843 - val_loss: 4.7017 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2407_52_42.683254\\model-00010-2.40681-0.28431-4.70174-0.15000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - 39s 1s/step - loss: 2.6275 - categorical_accuracy: 0.2157 - val_loss: 3.9624 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2407_52_42.683254\\model-00011-2.62745-0.21569-3.96240-0.18000.h5\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - 38s 1s/step - loss: 1.9579 - categorical_accuracy: 0.3333 - val_loss: 3.2582 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2407_52_42.683254\\model-00012-1.95789-0.33333-3.25818-0.20000.h5\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.7187 - categorical_accuracy: 0.2255 - val_loss: 2.5412 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2407_52_42.683254\\model-00013-1.71869-0.22549-2.54120-0.18000.h5\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.7832 - categorical_accuracy: 0.2549 - val_loss: 2.3658 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2407_52_42.683254\\model-00014-1.78324-0.25490-2.36582-0.17000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - 36s 1s/step - loss: 1.5489 - categorical_accuracy: 0.2843 - val_loss: 2.2511 - val_categorical_accuracy: 0.1300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2407_52_42.683254\\model-00015-1.54886-0.28431-2.25113-0.13000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a405ea30d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exp-2 and exp-3 there is very small change in training and validation accuracy even though we increase the no of layers and no of epochs.We can see the model is more impacted by image resolution,batch size and no of frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 4:change the image resolution to 80x80  ,with frames=30 and  by keeping batchsize=10 and epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 80 # image width\n",
    "z = 80 # image height\n",
    "\n",
    "batch_size=10\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b2 = Sequential()\n",
    "model_b2.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_b2.add(Activation('relu'))\n",
    "model_b2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "model_b2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_b2.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n",
    "model_b2.add(Activation('relu'))\n",
    "model_b2.add(BatchNormalization())\n",
    "\n",
    "model_b2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_b2.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_b2.add(Activation('relu'))\n",
    "model_b2.add(BatchNormalization())\n",
    "\n",
    "model_b2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_b2.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_b2.add(Activation('relu'))\n",
    "model_b2.add(BatchNormalization())\n",
    "\n",
    "model_b2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_b2.add(Flatten())\n",
    "model_b2.add(Dense(1000, activation='relu'))\n",
    "model_b2.add(Dropout(0.5))\n",
    "model_b2.add(Dense(500, activation='relu'))\n",
    "model_b2.add(Dropout(0.5))\n",
    "model_b2.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_13 (Conv3D)           (None, 30, 80, 80, 8)     656       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 30, 80, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 30, 80, 80, 8)     32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 15, 40, 40, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 15, 40, 40, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 15, 40, 40, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 15, 40, 40, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 7, 20, 20, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 7, 20, 20, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7, 20, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 7, 20, 20, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 3, 10, 10, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 3, 10, 10, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 3, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 3, 10, 10, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 5, 5, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1000)              1601000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 2,177,829\n",
      "Trainable params: 2,177,589\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model_b2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_b2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "65/67 [============================>.] - ETA: 3s - loss: 3.0387 - categorical_accuracy: 0.3554Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 3.0037 - categorical_accuracy: 0.3575Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 131s 2s/step - loss: 3.0037 - categorical_accuracy: 0.3575 - val_loss: 7.4109 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2407_52_42.683254\\model-00001-3.00365-0.35747-7.41094-0.21000.h5\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 54s 819ms/step - loss: 2.2505 - categorical_accuracy: 0.3433 - val_loss: 10.2956 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2407_52_42.683254\\model-00002-2.25049-0.34328-10.29561-0.22000.h5\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 53s 801ms/step - loss: 2.3427 - categorical_accuracy: 0.2488 - val_loss: 10.4931 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2407_52_42.683254\\model-00003-2.34273-0.24876-10.49308-0.19000.h5\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 54s 819ms/step - loss: 1.6969 - categorical_accuracy: 0.3781 - val_loss: 7.2922 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2407_52_42.683254\\model-00004-1.69692-0.37811-7.29221-0.25000.h5\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 53s 805ms/step - loss: 1.5876 - categorical_accuracy: 0.4179 - val_loss: 7.7782 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2407_52_42.683254\\model-00005-1.58764-0.41791-7.77822-0.24000.h5\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 55s 830ms/step - loss: 1.4294 - categorical_accuracy: 0.4826 - val_loss: 8.3638 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2407_52_42.683254\\model-00006-1.42938-0.48259-8.36381-0.18000.h5\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 53s 802ms/step - loss: 1.4341 - categorical_accuracy: 0.4876 - val_loss: 7.0081 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2407_52_42.683254\\model-00007-1.43407-0.48756-7.00811-0.18000.h5\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 56s 845ms/step - loss: 1.1222 - categorical_accuracy: 0.5721 - val_loss: 9.1726 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2407_52_42.683254\\model-00008-1.12220-0.57214-9.17258-0.20000.h5\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 54s 812ms/step - loss: 1.0653 - categorical_accuracy: 0.5572 - val_loss: 3.5569 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2407_52_42.683254\\model-00009-1.06526-0.55721-3.55688-0.30000.h5\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 55s 827ms/step - loss: 1.1946 - categorical_accuracy: 0.5821 - val_loss: 1.1747 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2407_52_42.683254\\model-00010-1.19456-0.58209-1.17470-0.48000.h5\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 53s 798ms/step - loss: 0.8750 - categorical_accuracy: 0.6517 - val_loss: 1.9100 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2407_52_42.683254\\model-00011-0.87504-0.65174-1.90999-0.41000.h5\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 53s 801ms/step - loss: 0.8514 - categorical_accuracy: 0.6816 - val_loss: 1.1036 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2407_52_42.683254\\model-00012-0.85140-0.68159-1.10360-0.60000.h5\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 54s 816ms/step - loss: 1.0137 - categorical_accuracy: 0.6766 - val_loss: 0.7827 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2407_52_42.683254\\model-00013-1.01371-0.67662-0.78271-0.69000.h5\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 54s 822ms/step - loss: 0.9089 - categorical_accuracy: 0.6866 - val_loss: 1.2933 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2407_52_42.683254\\model-00014-0.90889-0.68657-1.29328-0.56000.h5\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 56s 842ms/step - loss: 0.7654 - categorical_accuracy: 0.7313 - val_loss: 0.8183 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2407_52_42.683254\\model-00015-0.76545-0.73134-0.81835-0.69000.h5\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 53s 798ms/step - loss: 0.8281 - categorical_accuracy: 0.7065 - val_loss: 1.4481 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2407_52_42.683254\\model-00016-0.82805-0.70647-1.44809-0.46000.h5\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 56s 842ms/step - loss: 0.8872 - categorical_accuracy: 0.6617 - val_loss: 1.9721 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2407_52_42.683254\\model-00017-0.88721-0.66169-1.97206-0.52000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 52s 792ms/step - loss: 0.8051 - categorical_accuracy: 0.7065 - val_loss: 0.5977 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2407_52_42.683254\\model-00018-0.80512-0.70647-0.59772-0.80000.h5\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 52s 784ms/step - loss: 0.5046 - categorical_accuracy: 0.8308 - val_loss: 0.7200 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2407_52_42.683254\\model-00019-0.50461-0.83085-0.71999-0.72000.h5\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 56s 855ms/step - loss: 0.5373 - categorical_accuracy: 0.7910 - val_loss: 0.6156 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2407_52_42.683254\\model-00020-0.53726-0.79104-0.61563-0.79000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a405eb16d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exp-4 ,by increasing the trainable parameters,reducing the batch size  to 10,by changing the image dimensions,increasing the no of epochs ,we got the training accuracy and val accuracy are 79.1% and 79.0% respectively at the end of 20 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exp-5 : By changing  the image dimensions to 100x 100 ,batchsize=10 and epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 100 # image width\n",
    "z = 100 # image height\n",
    "batch_size=10\n",
    "num_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[x for x in range(0,30)]   #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size    # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = Sequential()\n",
    "\n",
    "model_c.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_c.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_c.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "model_c.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_c.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(BatchNormalization())\n",
    "\n",
    "model_c.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_c.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_c.add(Activation('relu'))\n",
    "model_c.add(BatchNormalization())\n",
    "\n",
    "model_c.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model_c.add(Flatten())\n",
    "model_c.add(Dense(1000, activation='relu'))\n",
    "model_c.add(Dropout(0.5))\n",
    "model_c.add(Dense(500, activation='relu'))\n",
    "model_c.add(Dropout(0.5))\n",
    "model_c.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_21 (Conv3D)           (None, 30, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 30, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 30, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 15, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 15, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 15, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 7, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 7, 25, 25, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 7, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              2305000   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 2,881,829\n",
      "Trainable params: 2,881,589\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam()\n",
    "model_c.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_c.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 10\n",
      "Epoch 1/30\n",
      "65/67 [============================>.] - ETA: 4s - loss: 2.8843 - categorical_accuracy: 0.4108Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.8636 - categorical_accuracy: 0.4118Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 161s 2s/step - loss: 2.8636 - categorical_accuracy: 0.4118 - val_loss: 4.7022 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2407_52_42.683254\\model-00001-2.86356-0.41176-4.70224-0.16000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 63s 949ms/step - loss: 2.1050 - categorical_accuracy: 0.4328 - val_loss: 2.9002 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2407_52_42.683254\\model-00002-2.10496-0.43284-2.90019-0.17000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 64s 974ms/step - loss: 2.2645 - categorical_accuracy: 0.3831 - val_loss: 5.3778 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2407_52_42.683254\\model-00003-2.26446-0.38308-5.37779-0.15000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 55s 838ms/step - loss: 1.6112 - categorical_accuracy: 0.5721 - val_loss: 4.8645 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2407_52_42.683254\\model-00004-1.61122-0.57214-4.86445-0.20000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 58s 882ms/step - loss: 1.2281 - categorical_accuracy: 0.5622 - val_loss: 3.5014 - val_categorical_accuracy: 0.1200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2407_52_42.683254\\model-00005-1.22808-0.56219-3.50140-0.12000.h5\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 56s 853ms/step - loss: 1.2173 - categorical_accuracy: 0.5423 - val_loss: 3.9009 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2407_52_42.683254\\model-00006-1.21734-0.54229-3.90085-0.22000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 58s 884ms/step - loss: 0.9633 - categorical_accuracy: 0.6418 - val_loss: 3.7665 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2407_52_42.683254\\model-00007-0.96329-0.64179-3.76651-0.18000.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 58s 875ms/step - loss: 0.6690 - categorical_accuracy: 0.7264 - val_loss: 2.7925 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2407_52_42.683254\\model-00008-0.66895-0.72637-2.79246-0.26000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 57s 866ms/step - loss: 0.6455 - categorical_accuracy: 0.7313 - val_loss: 1.9864 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2407_52_42.683254\\model-00009-0.64546-0.73134-1.98640-0.31000.h5\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 56s 854ms/step - loss: 0.6147 - categorical_accuracy: 0.7811 - val_loss: 1.2945 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2407_52_42.683254\\model-00010-0.61472-0.78109-1.29448-0.49000.h5\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 58s 881ms/step - loss: 0.5950 - categorical_accuracy: 0.8010 - val_loss: 0.6023 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2407_52_42.683254\\model-00011-0.59500-0.80100-0.60231-0.76000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 59s 893ms/step - loss: 0.4669 - categorical_accuracy: 0.8010 - val_loss: 0.5005 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2407_52_42.683254\\model-00012-0.46688-0.80100-0.50050-0.79000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 56s 847ms/step - loss: 0.4216 - categorical_accuracy: 0.8159 - val_loss: 0.5271 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2407_52_42.683254\\model-00013-0.42159-0.81592-0.52707-0.84000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 57s 867ms/step - loss: 0.4835 - categorical_accuracy: 0.8259 - val_loss: 0.4948 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2407_52_42.683254\\model-00014-0.48351-0.82587-0.49480-0.86000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 56s 846ms/step - loss: 0.3504 - categorical_accuracy: 0.8955 - val_loss: 0.3921 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2407_52_42.683254\\model-00015-0.35040-0.89552-0.39208-0.85000.h5\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 55s 839ms/step - loss: 0.3833 - categorical_accuracy: 0.8557 - val_loss: 0.5436 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2407_52_42.683254\\model-00016-0.38332-0.85572-0.54364-0.82000.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 59s 897ms/step - loss: 0.4007 - categorical_accuracy: 0.8507 - val_loss: 0.4454 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2407_52_42.683254\\model-00017-0.40070-0.85075-0.44544-0.87000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 58s 881ms/step - loss: 0.3042 - categorical_accuracy: 0.8955 - val_loss: 0.4372 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2407_52_42.683254\\model-00018-0.30418-0.89552-0.43721-0.89000.h5\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 56s 853ms/step - loss: 0.3613 - categorical_accuracy: 0.8657 - val_loss: 0.3797 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2407_52_42.683254\\model-00019-0.36134-0.86567-0.37967-0.90000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 57s 870ms/step - loss: 0.2363 - categorical_accuracy: 0.9154 - val_loss: 0.4202 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2407_52_42.683254\\model-00020-0.23627-0.91542-0.42020-0.85000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 58s 873ms/step - loss: 0.3391 - categorical_accuracy: 0.8856 - val_loss: 0.3880 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-10-2407_52_42.683254\\model-00021-0.33907-0.88557-0.38800-0.88000.h5\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 57s 868ms/step - loss: 0.2003 - categorical_accuracy: 0.9154 - val_loss: 0.5581 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-10-2407_52_42.683254\\model-00022-0.20028-0.91542-0.55814-0.83000.h5\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 54s 821ms/step - loss: 0.2732 - categorical_accuracy: 0.8856 - val_loss: 0.3359 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-10-2407_52_42.683254\\model-00023-0.27324-0.88557-0.33593-0.87000.h5\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 62s 931ms/step - loss: 0.2155 - categorical_accuracy: 0.9254 - val_loss: 0.3769 - val_categorical_accuracy: 0.9200\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-10-2407_52_42.683254\\model-00024-0.21554-0.92537-0.37689-0.92000.h5\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 57s 861ms/step - loss: 0.2271 - categorical_accuracy: 0.9154 - val_loss: 0.3700 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-10-2407_52_42.683254\\model-00025-0.22712-0.91542-0.36997-0.85000.h5\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 53s 798ms/step - loss: 0.1562 - categorical_accuracy: 0.9453 - val_loss: 0.4923 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00026: saving model to model_init_2021-10-2407_52_42.683254\\model-00026-0.15616-0.94527-0.49225-0.83000.h5\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 60s 906ms/step - loss: 0.1946 - categorical_accuracy: 0.9104 - val_loss: 0.3348 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00027: saving model to model_init_2021-10-2407_52_42.683254\\model-00027-0.19457-0.91045-0.33483-0.88000.h5\n",
      "Epoch 28/30\n",
      "67/67 [==============================] - 61s 918ms/step - loss: 0.2055 - categorical_accuracy: 0.9154 - val_loss: 0.4702 - val_categorical_accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: saving model to model_init_2021-10-2407_52_42.683254\\model-00028-0.20547-0.91542-0.47020-0.84000.h5\n",
      "Epoch 29/30\n",
      "67/67 [==============================] - 54s 821ms/step - loss: 0.1510 - categorical_accuracy: 0.9502 - val_loss: 0.4281 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00029: saving model to model_init_2021-10-2407_52_42.683254\\model-00029-0.15099-0.95025-0.42815-0.88000.h5\n",
      "Epoch 30/30\n",
      "67/67 [==============================] - 57s 865ms/step - loss: 0.2025 - categorical_accuracy: 0.9055 - val_loss: 0.3388 - val_categorical_accuracy: 0.9200\n",
      "\n",
      "Epoch 00030: saving model to model_init_2021-10-2407_52_42.683254\\model-00030-0.20252-0.90547-0.33880-0.92000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4efd88700>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exp-5, we increase the image dimensions to 100x100,the trainable parameters are increased ,hence we got the best results in this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-6 :Lets decrease the number of parameters by keeping batch size and epochs as same.That is by decreasing the dense neuron layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "num_epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c1 = Sequential()\n",
    "\n",
    "model_c1.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_c1.add(Activation('relu'))\n",
    "model_c1.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_c1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_c1.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_c1.add(Activation('relu'))\n",
    "model_c1.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "model_c1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_c1.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_c1.add(Activation('relu'))\n",
    "model_c1.add(BatchNormalization())\n",
    "\n",
    "model_c1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_c1.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_c1.add(Activation('relu'))\n",
    "model_c1.add(BatchNormalization())\n",
    "\n",
    "model_c1.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model_c1.add(Flatten())\n",
    "model_c1.add(Dense(256, activation='relu'))\n",
    "model_c1.add(Dropout(0.25))\n",
    "model_c1.add(Dense(128, activation='relu'))\n",
    "model_c1.add(Dropout(0.25))\n",
    "model_c1.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_29 (Conv3D)           (None, 30, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 30, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 30, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 15, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 15, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 15, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 7, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 7, 25, 25, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 7, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_32 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 697,445\n",
      "Trainable params: 697,205\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_c1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_c1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 10\n",
      "Epoch 1/30\n",
      "65/67 [============================>.] - ETA: 3s - loss: 1.9533 - categorical_accuracy: 0.3508Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.9381 - categorical_accuracy: 0.3514Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 149s 2s/step - loss: 1.9381 - categorical_accuracy: 0.3514 - val_loss: 4.8139 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2407_52_42.683254\\model-00001-1.93815-0.35143-4.81388-0.21000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 58s 881ms/step - loss: 1.8112 - categorical_accuracy: 0.3682 - val_loss: 9.2478 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2407_52_42.683254\\model-00002-1.81124-0.36816-9.24781-0.18000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 58s 872ms/step - loss: 1.6059 - categorical_accuracy: 0.3781 - val_loss: 6.1146 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2407_52_42.683254\\model-00003-1.60591-0.37811-6.11457-0.23000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 57s 863ms/step - loss: 1.4983 - categorical_accuracy: 0.4627 - val_loss: 4.5898 - val_categorical_accuracy: 0.1400\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2407_52_42.683254\\model-00004-1.49827-0.46269-4.58979-0.14000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 55s 827ms/step - loss: 1.2911 - categorical_accuracy: 0.5174 - val_loss: 5.1339 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2407_52_42.683254\\model-00005-1.29107-0.51741-5.13388-0.15000.h5\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 58s 880ms/step - loss: 1.2980 - categorical_accuracy: 0.5274 - val_loss: 4.5942 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2407_52_42.683254\\model-00006-1.29795-0.52736-4.59421-0.21000.h5\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 57s 862ms/step - loss: 1.0510 - categorical_accuracy: 0.5871 - val_loss: 3.6385 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2407_52_42.683254\\model-00007-1.05103-0.58706-3.63846-0.18000.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 58s 884ms/step - loss: 0.8807 - categorical_accuracy: 0.6318 - val_loss: 3.0368 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2407_52_42.683254\\model-00008-0.88067-0.63184-3.03682-0.22000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 59s 890ms/step - loss: 0.9403 - categorical_accuracy: 0.6269 - val_loss: 2.0069 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2407_52_42.683254\\model-00009-0.94029-0.62687-2.00694-0.38000.h5\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 59s 894ms/step - loss: 0.9836 - categorical_accuracy: 0.6468 - val_loss: 3.0028 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2407_52_42.683254\\model-00010-0.98359-0.64677-3.00284-0.17000.h5\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 56s 854ms/step - loss: 0.9571 - categorical_accuracy: 0.6219 - val_loss: 0.9663 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2407_52_42.683254\\model-00011-0.95709-0.62189-0.96633-0.61000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 58s 881ms/step - loss: 0.8121 - categorical_accuracy: 0.6816 - val_loss: 0.7990 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2407_52_42.683254\\model-00012-0.81208-0.68159-0.79903-0.71000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 58s 879ms/step - loss: 0.6592 - categorical_accuracy: 0.7463 - val_loss: 2.3373 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2407_52_42.683254\\model-00013-0.65917-0.74627-2.33725-0.46000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 58s 880ms/step - loss: 0.6921 - categorical_accuracy: 0.7861 - val_loss: 0.6353 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2407_52_42.683254\\model-00014-0.69205-0.78607-0.63526-0.81000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 57s 869ms/step - loss: 0.7971 - categorical_accuracy: 0.6965 - val_loss: 1.1578 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2407_52_42.683254\\model-00015-0.79710-0.69652-1.15780-0.47000.h5\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 58s 875ms/step - loss: 0.4402 - categorical_accuracy: 0.8308 - val_loss: 0.6163 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2407_52_42.683254\\model-00016-0.44017-0.83085-0.61625-0.78000.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 81s 1s/step - loss: 0.6181 - categorical_accuracy: 0.7761 - val_loss: 0.5976 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2407_52_42.683254\\model-00017-0.61812-0.77612-0.59757-0.79000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.4280 - categorical_accuracy: 0.8209 - val_loss: 1.1202 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2407_52_42.683254\\model-00018-0.42800-0.82090-1.12022-0.69000.h5\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 78s 1s/step - loss: 0.4507 - categorical_accuracy: 0.8557 - val_loss: 0.8635 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2407_52_42.683254\\model-00019-0.45075-0.85572-0.86347-0.76000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.5199 - categorical_accuracy: 0.8060 - val_loss: 0.6462 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2407_52_42.683254\\model-00020-0.51988-0.80597-0.64621-0.77000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 86s 1s/step - loss: 0.5104 - categorical_accuracy: 0.8159 - val_loss: 0.6786 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-10-2407_52_42.683254\\model-00021-0.51041-0.81592-0.67861-0.78000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.2376 - categorical_accuracy: 0.9204 - val_loss: 0.5616 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-10-2407_52_42.683254\\model-00022-0.23763-0.92040-0.56160-0.83000.h5\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 71s 1s/step - loss: 0.2632 - categorical_accuracy: 0.8955 - val_loss: 0.5135 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-10-2407_52_42.683254\\model-00023-0.26317-0.89552-0.51354-0.88000.h5\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 65s 987ms/step - loss: 0.1702 - categorical_accuracy: 0.9502 - val_loss: 0.4695 - val_categorical_accuracy: 0.8900\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-10-2407_52_42.683254\\model-00024-0.17018-0.95025-0.46953-0.89000.h5\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 63s 958ms/step - loss: 0.2961 - categorical_accuracy: 0.9005 - val_loss: 0.5101 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-10-2407_52_42.683254\\model-00025-0.29614-0.90050-0.51013-0.88000.h5\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.1229 - categorical_accuracy: 0.9652 - val_loss: 0.5131 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00026: saving model to model_init_2021-10-2407_52_42.683254\\model-00026-0.12293-0.96517-0.51307-0.86000.h5\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 61s 929ms/step - loss: 0.1775 - categorical_accuracy: 0.9403 - val_loss: 0.5163 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00027: saving model to model_init_2021-10-2407_52_42.683254\\model-00027-0.17754-0.94030-0.51631-0.88000.h5\n",
      "Epoch 28/30\n",
      "67/67 [==============================] - 60s 904ms/step - loss: 0.1863 - categorical_accuracy: 0.9353 - val_loss: 0.2773 - val_categorical_accuracy: 0.9400\n",
      "\n",
      "Epoch 00028: saving model to model_init_2021-10-2407_52_42.683254\\model-00028-0.18628-0.93532-0.27730-0.94000.h5\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 62s 943ms/step - loss: 0.1290 - categorical_accuracy: 0.9552 - val_loss: 0.5541 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00029: saving model to model_init_2021-10-2407_52_42.683254\\model-00029-0.12901-0.95522-0.55411-0.86000.h5\n",
      "Epoch 30/30\n",
      "67/67 [==============================] - 64s 966ms/step - loss: 0.1629 - categorical_accuracy: 0.9403 - val_loss: 0.4238 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00030: saving model to model_init_2021-10-2407_52_42.683254\\model-00030-0.16293-0.94030-0.42383-0.88000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4f24dd580>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results clearly shows that when we use batch size=10 and also by having the trainable parametes decreased ,the training and validation accuracy are 94.03% and 88.0% respectively at the end of 30 epochs.<br>Since we use batch size=10 ,the iterations are more and it takes more computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exp-7 :By  increasing the batchsize to 20 and rest remaining same.let us try to examine the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "\n",
    "model_c2 = Sequential()\n",
    "\n",
    "model_c2.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_c2.add(Activation('relu'))\n",
    "model_c2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_c2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model_c2.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_c2.add(Activation('relu'))\n",
    "model_c2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "model_c2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_c2.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_c2.add(Activation('relu'))\n",
    "model_c2.add(BatchNormalization())\n",
    "\n",
    "model_c2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "model_c2.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_c2.add(Activation('relu'))\n",
    "model_c2.add(BatchNormalization())\n",
    "\n",
    "model_c2.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model_c2.add(Flatten())\n",
    "model_c2.add(Dense(256, activation='relu'))\n",
    "model_c2.add(Dropout(0.25))\n",
    "model_c2.add(Dense(128, activation='relu'))\n",
    "model_c2.add(Dropout(0.25))\n",
    "model_c2.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_33 (Conv3D)           (None, 30, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 30, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 30, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_29 (MaxPooling (None, 15, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_34 (Conv3D)           (None, 15, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 15, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_30 (MaxPooling (None, 7, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_35 (Conv3D)           (None, 7, 25, 25, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 7, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_31 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_32 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 697,445\n",
      "Trainable params: 697,205\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_c2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_c2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 20\n",
      "Epoch 1/30\n",
      "32/34 [===========================>..] - ETA: 12s - loss: 1.8657 - categorical_accuracy: 0.3797Batch:  34 Index: 20\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.8507 - categorical_accuracy: 0.3876Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 228s 7s/step - loss: 1.8507 - categorical_accuracy: 0.3876 - val_loss: 2.8292 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2407_52_42.683254\\model-00001-1.85071-0.38763-2.82922-0.21000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.6230 - categorical_accuracy: 0.4020 - val_loss: 4.5205 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2407_52_42.683254\\model-00002-1.62302-0.40196-4.52054-0.23000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 41s 1s/step - loss: 2.0491 - categorical_accuracy: 0.3039 - val_loss: 6.5272 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2407_52_42.683254\\model-00003-2.04910-0.30392-6.52720-0.18000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.9381 - categorical_accuracy: 0.3235 - val_loss: 5.2112 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2407_52_42.683254\\model-00004-1.93807-0.32353-5.21118-0.21000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.6637 - categorical_accuracy: 0.3529 - val_loss: 4.4309 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2407_52_42.683254\\model-00005-1.66374-0.35294-4.43092-0.24000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.7361 - categorical_accuracy: 0.3824 - val_loss: 3.7527 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2407_52_42.683254\\model-00006-1.73611-0.38235-3.75270-0.21000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.3744 - categorical_accuracy: 0.4510 - val_loss: 4.0590 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2407_52_42.683254\\model-00007-1.37444-0.45098-4.05896-0.24000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 40s 1s/step - loss: 1.1817 - categorical_accuracy: 0.5686 - val_loss: 4.8544 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2407_52_42.683254\\model-00008-1.18167-0.56863-4.85437-0.16000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.9540 - categorical_accuracy: 0.5882 - val_loss: 5.5332 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2407_52_42.683254\\model-00009-0.95400-0.58824-5.53325-0.25000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.0424 - categorical_accuracy: 0.5294 - val_loss: 6.2968 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2407_52_42.683254\\model-00010-1.04236-0.52941-6.29684-0.19000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.0341 - categorical_accuracy: 0.5980 - val_loss: 6.0671 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2407_52_42.683254\\model-00011-1.03415-0.59804-6.06708-0.21000.h5\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4f26c74c0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exp-7 ,we see that as the batch size increase to 20,we saw the model overfit and the validation loss is not decreasing,and we saw the early stopping at the 11th epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-8 : Add dropout layers to the cnn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c3 = Sequential()\n",
    "\n",
    "model_c3.add(Conv3D(8, kernel_size=(3, 3, 3), input_shape=(x,y,z,channels), padding='same'))\n",
    "model_c3.add(Activation('relu'))\n",
    "model_c3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_c3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_c3.add(Dropout(0.25))\n",
    "\n",
    "model_c3.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same'))\n",
    "model_c3.add(Activation('relu'))\n",
    "model_c3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "model_c3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_c3.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_c3.add(Conv3D(32, kernel_size=(3,3,3), padding='same'))\n",
    "model_c3.add(Activation('relu'))\n",
    "model_c3.add(BatchNormalization())\n",
    "\n",
    "model_c3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_c3.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_c3.add(Conv3D(64, kernel_size=(3,3,3), padding='same'))\n",
    "model_c3.add(Activation('relu'))\n",
    "model_c3.add(BatchNormalization())\n",
    "\n",
    "model_c3.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_c3.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "model_c3.add(Flatten())\n",
    "model_c3.add(Dense(256, activation='relu'))\n",
    "model_c3.add(Dropout(0.25))\n",
    "model_c3.add(Dense(128, activation='relu'))\n",
    "model_c3.add(Dropout(0.25))\n",
    "model_c3.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_49 (Conv3D)           (None, 30, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 30, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 30, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_45 (MaxPooling (None, 15, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 15, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_50 (Conv3D)           (None, 15, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 15, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_46 (MaxPooling (None, 7, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 7, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_51 (Conv3D)           (None, 7, 25, 25, 32)     13856     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 7, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_47 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_52 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_48 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 697,445\n",
      "Trainable params: 697,205\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_c3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_c3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 20\n",
      "Epoch 1/30\n",
      "32/34 [===========================>..] - ETA: 7s - loss: 2.6084 - categorical_accuracy: 0.2875 Batch:  34 Index: 20\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.5880 - categorical_accuracy: 0.2866Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 20\n",
      "34/34 [==============================] - 147s 4s/step - loss: 2.5880 - categorical_accuracy: 0.2866 - val_loss: 2.7810 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2407_52_42.683254\\model-00001-2.58797-0.28658-2.78104-0.16000.h5\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.9498 - categorical_accuracy: 0.3235 - val_loss: 4.6998 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2407_52_42.683254\\model-00002-1.94981-0.32353-4.69980-0.18000.h5\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.7547 - categorical_accuracy: 0.3529 - val_loss: 3.5545 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2407_52_42.683254\\model-00003-1.75467-0.35294-3.55451-0.17000.h5\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.6846 - categorical_accuracy: 0.3529 - val_loss: 3.9690 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2407_52_42.683254\\model-00004-1.68464-0.35294-3.96897-0.16000.h5\n",
      "Epoch 5/30\n",
      "34/34 [==============================] - 43s 1s/step - loss: 1.8571 - categorical_accuracy: 0.3529 - val_loss: 3.3314 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2407_52_42.683254\\model-00005-1.85709-0.35294-3.33143-0.25000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/30\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.5662 - categorical_accuracy: 0.3824 - val_loss: 3.9606 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2407_52_42.683254\\model-00006-1.56617-0.38235-3.96056-0.21000.h5\n",
      "Epoch 7/30\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.4755 - categorical_accuracy: 0.4216 - val_loss: 4.4009 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2407_52_42.683254\\model-00007-1.47554-0.42157-4.40095-0.18000.h5\n",
      "Epoch 8/30\n",
      "34/34 [==============================] - 44s 1s/step - loss: 1.4701 - categorical_accuracy: 0.3725 - val_loss: 4.4636 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2407_52_42.683254\\model-00008-1.47008-0.37255-4.46359-0.22000.h5\n",
      "Epoch 9/30\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.3627 - categorical_accuracy: 0.4510 - val_loss: 3.9450 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2407_52_42.683254\\model-00009-1.36272-0.45098-3.94504-0.22000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/30\n",
      "34/34 [==============================] - 47s 1s/step - loss: 1.4092 - categorical_accuracy: 0.4020 - val_loss: 4.2971 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2407_52_42.683254\\model-00010-1.40920-0.40196-4.29713-0.23000.h5\n",
      "Epoch 11/30\n",
      "34/34 [==============================] - 45s 1s/step - loss: 1.3664 - categorical_accuracy: 0.4020 - val_loss: 4.3388 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2407_52_42.683254\\model-00011-1.36644-0.40196-4.33876-0.21000.h5\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4efdc3340>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Even after adding drop out layers the training accuracy has reduced ,but the model still overfits as shown in the accuracies above.The validation loss is not decreasing and the model stops learning at 11th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hence the final model is Exp-6 with batch size 10 and with the training and validation accuracy are 94.03% and 88.0% respectively for conv 3d  model with least no of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Conv2D + RNN to build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a custom conv2d+ rnn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 100 # image width\n",
    "z = 100 # image height\n",
    "batch_size=10\n",
    "num_epochs=30\n",
    "channels=3\n",
    "classes=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[x for x in range(0,x)]   #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size    # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = \"C:\\cnndatasets\\Project_data/train\"\n",
    "val_path = \"C:\\cnndatasets\\Project_data/val\"\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs =   30  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_batches = num_train_sequences//batch_size \n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp- 9 : Using custom conv2d + GRU to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "input_shape=(x,y,z,channels)\n",
    "model_d= Sequential()\n",
    "\n",
    "model_d.add(TimeDistributed(Conv2D(16, kernel_size=(3, 3),  padding='same'),input_shape=input_shape))\n",
    "model_d.add(TimeDistributed(Activation('relu')))\n",
    "model_d.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d.add(TimeDistributed(Conv2D(32, kernel_size=(3, 3), padding='same')))\n",
    "model_d.add(TimeDistributed(Activation('relu')))\n",
    "model_d.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d.add(TimeDistributed(Conv2D(64, kernel_size=(3, 3),  padding='same')))\n",
    "model_d.add(TimeDistributed(Activation('relu')))\n",
    "model_d.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d.add(TimeDistributed(Conv2D(128, kernel_size=(3, 3), padding='same')))\n",
    "model_d.add(TimeDistributed(Activation('relu')))\n",
    "model_d.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_d.add(GRU(64))\n",
    "model_d.add(Dropout(0.25))\n",
    "\n",
    "model_d.add(Dense(64,activation='relu'))\n",
    "model_d.add(Dropout(0.25))\n",
    "\n",
    "model_d.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_54 (TimeDis (None, 30, 100, 100, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_55 (TimeDis (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_56 (TimeDis (None, 30, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_57 (TimeDis (None, 30, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_58 (TimeDis (None, 30, 50, 50, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, 30, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_60 (TimeDis (None, 30, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_61 (TimeDis (None, 30, 25, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_62 (TimeDis (None, 30, 25, 25, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_63 (TimeDis (None, 30, 25, 25, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_64 (TimeDis (None, 30, 25, 25, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_65 (TimeDis (None, 30, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_66 (TimeDis (None, 30, 12, 12, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_67 (TimeDis (None, 30, 12, 12, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_68 (TimeDis (None, 30, 12, 12, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_69 (TimeDis (None, 30, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_70 (TimeDis (None, 30, 4608)          0         \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 64)                897408    \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,000,293\n",
      "Trainable params: 999,813\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_d.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_d.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 10\n",
      "Epoch 1/30\n",
      "65/67 [============================>.] - ETA: 7s - loss: 1.5744 - categorical_accuracy: 0.2985 Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.5747 - categorical_accuracy: 0.3017Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 282s 4s/step - loss: 1.5747 - categorical_accuracy: 0.3017 - val_loss: 1.7328 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2521_05_10.942506\\model-00001-1.57467-0.30166-1.73278-0.21000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 57s 867ms/step - loss: 1.5183 - categorical_accuracy: 0.3731 - val_loss: 1.7572 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2521_05_10.942506\\model-00002-1.51832-0.37313-1.75715-0.20000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 56s 846ms/step - loss: 1.4300 - categorical_accuracy: 0.3781 - val_loss: 1.7898 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2521_05_10.942506\\model-00003-1.43003-0.37811-1.78984-0.21000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 60s 911ms/step - loss: 1.3420 - categorical_accuracy: 0.4428 - val_loss: 1.8510 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2521_05_10.942506\\model-00004-1.34199-0.44279-1.85099-0.20000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 60s 912ms/step - loss: 1.4068 - categorical_accuracy: 0.4129 - val_loss: 2.3859 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2521_05_10.942506\\model-00005-1.40677-0.41294-2.38586-0.19000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 57s 856ms/step - loss: 1.2955 - categorical_accuracy: 0.4527 - val_loss: 2.3552 - val_categorical_accuracy: 0.1200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2521_05_10.942506\\model-00006-1.29550-0.45274-2.35519-0.12000.h5\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 59s 898ms/step - loss: 1.2459 - categorical_accuracy: 0.4876 - val_loss: 2.1613 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2521_05_10.942506\\model-00007-1.24587-0.48756-2.16132-0.27000.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 56s 853ms/step - loss: 1.2872 - categorical_accuracy: 0.4826 - val_loss: 2.2776 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2521_05_10.942506\\model-00008-1.28722-0.48259-2.27759-0.18000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 58s 873ms/step - loss: 1.1412 - categorical_accuracy: 0.5423 - val_loss: 2.1650 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2521_05_10.942506\\model-00009-1.14119-0.54229-2.16497-0.29000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 56s 851ms/step - loss: 1.0939 - categorical_accuracy: 0.5920 - val_loss: 1.9886 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2521_05_10.942506\\model-00010-1.09389-0.59204-1.98860-0.33000.h5\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 57s 864ms/step - loss: 1.0999 - categorical_accuracy: 0.5522 - val_loss: 1.3505 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2521_05_10.942506\\model-00011-1.09991-0.55224-1.35047-0.51000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 60s 903ms/step - loss: 1.0133 - categorical_accuracy: 0.6119 - val_loss: 1.0913 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2521_05_10.942506\\model-00012-1.01325-0.61194-1.09130-0.63000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 57s 866ms/step - loss: 1.1416 - categorical_accuracy: 0.6020 - val_loss: 1.0451 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2521_05_10.942506\\model-00013-1.14162-0.60199-1.04506-0.63000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 57s 863ms/step - loss: 1.0633 - categorical_accuracy: 0.6318 - val_loss: 0.9323 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2521_05_10.942506\\model-00014-1.06330-0.63184-0.93229-0.73000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 56s 850ms/step - loss: 1.0064 - categorical_accuracy: 0.6368 - val_loss: 0.9942 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2521_05_10.942506\\model-00015-1.00641-0.63682-0.99417-0.72000.h5\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 61s 921ms/step - loss: 1.0171 - categorical_accuracy: 0.6269 - val_loss: 1.0325 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2521_05_10.942506\\model-00016-1.01711-0.62687-1.03247-0.63000.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 61s 928ms/step - loss: 0.9565 - categorical_accuracy: 0.6716 - val_loss: 0.9997 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2521_05_10.942506\\model-00017-0.95653-0.67164-0.99969-0.66000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 60s 914ms/step - loss: 1.1291 - categorical_accuracy: 0.5274 - val_loss: 0.9697 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2521_05_10.942506\\model-00018-1.12911-0.52736-0.96969-0.68000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 58s 879ms/step - loss: 0.9703 - categorical_accuracy: 0.6418 - val_loss: 1.0418 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2521_05_10.942506\\model-00019-0.97026-0.64179-1.04183-0.65000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 57s 864ms/step - loss: 0.9719 - categorical_accuracy: 0.6269 - val_loss: 0.9532 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2521_05_10.942506\\model-00020-0.97188-0.62687-0.95319-0.68000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 60s 909ms/step - loss: 1.0778 - categorical_accuracy: 0.5572 - val_loss: 0.9862 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-10-2521_05_10.942506\\model-00021-1.07778-0.55721-0.98615-0.69000.h5\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 1.0373 - categorical_accuracy: 0.5871 - val_loss: 0.9767 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-10-2521_05_10.942506\\model-00022-1.03730-0.58706-0.97672-0.69000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 55s 836ms/step - loss: 0.9465 - categorical_accuracy: 0.6219 - val_loss: 0.9459 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-10-2521_05_10.942506\\model-00023-0.94648-0.62189-0.94588-0.68000.h5\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 58s 877ms/step - loss: 1.0317 - categorical_accuracy: 0.6318 - val_loss: 0.9862 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-10-2521_05_10.942506\\model-00024-1.03174-0.63184-0.98621-0.69000.h5\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4f3632cd0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above model we can infer that the accuracy of training and validation are 63.14% and 69.0% respectively.<br> The model stops at 24th epoch as the validation loss does not decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-10 : Let us add more dense nuerons and gru cells and examine the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(x,y,z,channels)\n",
    "model_d1= Sequential()\n",
    "\n",
    "model_d1.add(TimeDistributed(Conv2D(16, kernel_size=(3, 3),  padding='same'),input_shape=input_shape))\n",
    "model_d1.add(TimeDistributed(Activation('relu')))\n",
    "model_d1.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d1.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d1.add(TimeDistributed(Conv2D(32, kernel_size=(3, 3), padding='same')))\n",
    "model_d1.add(TimeDistributed(Activation('relu')))\n",
    "model_d1.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d1.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d1.add(TimeDistributed(Conv2D(64, kernel_size=(3, 3),  padding='same')))\n",
    "model_d1.add(TimeDistributed(Activation('relu')))\n",
    "model_d1.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d1.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d1.add(TimeDistributed(Conv2D(128, kernel_size=(3, 3), padding='same')))\n",
    "model_d1.add(TimeDistributed(Activation('relu')))\n",
    "model_d1.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d1.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d1.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_d1.add(GRU(128))\n",
    "model_d1.add(Dropout(0.25))\n",
    "\n",
    "model_d1.add(Dense(128,activation='relu'))\n",
    "model_d1.add(Dropout(0.25))\n",
    "\n",
    "model_d1.add(Dense(classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_71 (TimeDis (None, 30, 100, 100, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_72 (TimeDis (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_73 (TimeDis (None, 30, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_74 (TimeDis (None, 30, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_75 (TimeDis (None, 30, 50, 50, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_76 (TimeDis (None, 30, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_77 (TimeDis (None, 30, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_78 (TimeDis (None, 30, 25, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_79 (TimeDis (None, 30, 25, 25, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_80 (TimeDis (None, 30, 25, 25, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_81 (TimeDis (None, 30, 25, 25, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_82 (TimeDis (None, 30, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_83 (TimeDis (None, 30, 12, 12, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_84 (TimeDis (None, 30, 12, 12, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_85 (TimeDis (None, 30, 12, 12, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_86 (TimeDis (None, 30, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_87 (TimeDis (None, 30, 4608)          0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 128)               1819392   \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,934,949\n",
      "Trainable params: 1,934,469\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_d1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_d1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 10\n",
      "Epoch 1/30\n",
      "65/67 [============================>.] - ETA: 3s - loss: 1.4856 - categorical_accuracy: 0.3585Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4791 - categorical_accuracy: 0.3635Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 142s 2s/step - loss: 1.4791 - categorical_accuracy: 0.3635 - val_loss: 3.3977 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2521_05_10.942506\\model-00001-1.47912-0.36350-3.39767-0.18000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 56s 852ms/step - loss: 1.4506 - categorical_accuracy: 0.3731 - val_loss: 2.1774 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2521_05_10.942506\\model-00002-1.45055-0.37313-2.17741-0.16000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 58s 871ms/step - loss: 1.4209 - categorical_accuracy: 0.4627 - val_loss: 2.2510 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2521_05_10.942506\\model-00003-1.42087-0.46269-2.25097-0.15000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 58s 874ms/step - loss: 1.4518 - categorical_accuracy: 0.3731 - val_loss: 2.2552 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2521_05_10.942506\\model-00004-1.45185-0.37313-2.25518-0.19000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 59s 894ms/step - loss: 1.3028 - categorical_accuracy: 0.4527 - val_loss: 2.4340 - val_categorical_accuracy: 0.1200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2521_05_10.942506\\model-00005-1.30278-0.45274-2.43397-0.12000.h5\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 56s 850ms/step - loss: 1.4210 - categorical_accuracy: 0.3881 - val_loss: 2.9585 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2521_05_10.942506\\model-00006-1.42101-0.38806-2.95852-0.21000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 59s 894ms/step - loss: 1.2020 - categorical_accuracy: 0.5274 - val_loss: 2.7367 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2521_05_10.942506\\model-00007-1.20201-0.52736-2.73670-0.15000.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 56s 843ms/step - loss: 1.1303 - categorical_accuracy: 0.5522 - val_loss: 2.5555 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2521_05_10.942506\\model-00008-1.13034-0.55224-2.55551-0.15000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 58s 871ms/step - loss: 1.0359 - categorical_accuracy: 0.6169 - val_loss: 2.5741 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2521_05_10.942506\\model-00009-1.03587-0.61692-2.57411-0.30000.h5\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 56s 851ms/step - loss: 1.0523 - categorical_accuracy: 0.6020 - val_loss: 2.3562 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2521_05_10.942506\\model-00010-1.05225-0.60199-2.35617-0.28000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 58s 881ms/step - loss: 1.0004 - categorical_accuracy: 0.6219 - val_loss: 1.8741 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2521_05_10.942506\\model-00011-1.00037-0.62189-1.87406-0.36000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 58s 883ms/step - loss: 0.9169 - categorical_accuracy: 0.6816 - val_loss: 1.2933 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2521_05_10.942506\\model-00012-0.91687-0.68159-1.29330-0.49000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 59s 889ms/step - loss: 0.9260 - categorical_accuracy: 0.6269 - val_loss: 0.8928 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2521_05_10.942506\\model-00013-0.92596-0.62687-0.89278-0.71000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 56s 844ms/step - loss: 0.8578 - categorical_accuracy: 0.6915 - val_loss: 0.8252 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2521_05_10.942506\\model-00014-0.85779-0.69154-0.82516-0.73000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 58s 876ms/step - loss: 0.8264 - categorical_accuracy: 0.6766 - val_loss: 0.9070 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2521_05_10.942506\\model-00015-0.82644-0.67662-0.90702-0.66000.h5\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 58s 884ms/step - loss: 0.8729 - categorical_accuracy: 0.6965 - val_loss: 0.7910 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2521_05_10.942506\\model-00016-0.87288-0.69652-0.79105-0.71000.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 57s 865ms/step - loss: 0.8861 - categorical_accuracy: 0.6716 - val_loss: 0.8332 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2521_05_10.942506\\model-00017-0.88612-0.67164-0.83316-0.72000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 58s 871ms/step - loss: 0.9647 - categorical_accuracy: 0.6368 - val_loss: 0.7436 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2521_05_10.942506\\model-00018-0.96473-0.63682-0.74365-0.76000.h5\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 59s 893ms/step - loss: 0.8671 - categorical_accuracy: 0.6915 - val_loss: 0.8538 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2521_05_10.942506\\model-00019-0.86712-0.69154-0.85377-0.72000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 57s 869ms/step - loss: 0.7707 - categorical_accuracy: 0.7363 - val_loss: 0.7782 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2521_05_10.942506\\model-00020-0.77069-0.73632-0.77817-0.76000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 55s 828ms/step - loss: 0.8035 - categorical_accuracy: 0.7313 - val_loss: 0.8408 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-10-2521_05_10.942506\\model-00021-0.80350-0.73134-0.84081-0.71000.h5\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 62s 936ms/step - loss: 0.7777 - categorical_accuracy: 0.7114 - val_loss: 0.8160 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-10-2521_05_10.942506\\model-00022-0.77767-0.71144-0.81596-0.76000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 59s 889ms/step - loss: 0.7515 - categorical_accuracy: 0.7463 - val_loss: 0.7960 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-10-2521_05_10.942506\\model-00023-0.75152-0.74627-0.79603-0.76000.h5\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 56s 848ms/step - loss: 0.8306 - categorical_accuracy: 0.6816 - val_loss: 0.7768 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-10-2521_05_10.942506\\model-00024-0.83063-0.68159-0.77678-0.73000.h5\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.7234 - categorical_accuracy: 0.7612 - val_loss: 0.7316 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-10-2521_05_10.942506\\model-00025-0.72342-0.76119-0.73163-0.79000.h5\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 56s 841ms/step - loss: 0.8780 - categorical_accuracy: 0.6965 - val_loss: 0.8251 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00026: saving model to model_init_2021-10-2521_05_10.942506\\model-00026-0.87800-0.69652-0.82515-0.73000.h5\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.7905 - categorical_accuracy: 0.7463 - val_loss: 0.7951 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00027: saving model to model_init_2021-10-2521_05_10.942506\\model-00027-0.79049-0.74627-0.79510-0.76000.h5\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 58s 875ms/step - loss: 0.8391 - categorical_accuracy: 0.6716 - val_loss: 0.7715 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00028: saving model to model_init_2021-10-2521_05_10.942506\\model-00028-0.83908-0.67164-0.77149-0.72000.h5\n",
      "Epoch 29/30\n",
      "67/67 [==============================] - 57s 869ms/step - loss: 0.7255 - categorical_accuracy: 0.7562 - val_loss: 0.7503 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00029: saving model to model_init_2021-10-2521_05_10.942506\\model-00029-0.72554-0.75622-0.75027-0.76000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 30/30\n",
      "67/67 [==============================] - 59s 900ms/step - loss: 0.7712 - categorical_accuracy: 0.7214 - val_loss: 0.8669 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00030: saving model to model_init_2021-10-2521_05_10.942506\\model-00030-0.77122-0.72139-0.86692-0.69000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4f36328e0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above experiment we saw an in increase in training accuracy but no increase in validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-11: lets add layers in GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(x,y,z,channels)\n",
    "model_d2= Sequential()\n",
    "\n",
    "model_d2.add(TimeDistributed(Conv2D(16, kernel_size=(3, 3),  padding='same'),input_shape=input_shape))\n",
    "model_d2.add(TimeDistributed(Activation('relu')))\n",
    "model_d2.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d2.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d2.add(TimeDistributed(Conv2D(32, kernel_size=(3, 3), padding='same')))\n",
    "model_d2.add(TimeDistributed(Activation('relu')))\n",
    "model_d2.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d2.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d2.add(TimeDistributed(Conv2D(64, kernel_size=(3, 3),  padding='same')))\n",
    "model_d2.add(TimeDistributed(Activation('relu')))\n",
    "model_d2.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d2.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d2.add(TimeDistributed(Conv2D(128, kernel_size=(3, 3), padding='same')))\n",
    "model_d2.add(TimeDistributed(Activation('relu')))\n",
    "model_d2.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d2.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d2.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_d2.add(GRU(128,return_sequences=True))\n",
    "model_d2.add(Dropout(0.25))\n",
    "\n",
    "model_d2.add(GRU(128))\n",
    "model_d2.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_d2.add(Dense(128,activation='relu'))\n",
    "model_d2.add(Dropout(0.25))\n",
    "\n",
    "model_d2.add(Dense(classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_156 (TimeDi (None, 30, 100, 100, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_157 (TimeDi (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_158 (TimeDi (None, 30, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_159 (TimeDi (None, 30, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_160 (TimeDi (None, 30, 50, 50, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_161 (TimeDi (None, 30, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_162 (TimeDi (None, 30, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_163 (TimeDi (None, 30, 25, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_164 (TimeDi (None, 30, 25, 25, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_165 (TimeDi (None, 30, 25, 25, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_166 (TimeDi (None, 30, 25, 25, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_167 (TimeDi (None, 30, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_168 (TimeDi (None, 30, 12, 12, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_169 (TimeDi (None, 30, 12, 12, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_170 (TimeDi (None, 30, 12, 12, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_171 (TimeDi (None, 30, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_172 (TimeDi (None, 30, 4608)          0         \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 30, 128)           1819392   \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_21 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,034,021\n",
      "Trainable params: 2,033,541\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_d2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_d2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 10\n",
      "Epoch 1/30\n",
      "65/67 [============================>.] - ETA: 3s - loss: 1.4475 - categorical_accuracy: 0.3538Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4454 - categorical_accuracy: 0.3544Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 144s 2s/step - loss: 1.4454 - categorical_accuracy: 0.3544 - val_loss: 1.9388 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2521_05_10.942506\\model-00001-1.44541-0.35445-1.93876-0.16000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 60s 906ms/step - loss: 1.3873 - categorical_accuracy: 0.4577 - val_loss: 1.6977 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2521_05_10.942506\\model-00002-1.38729-0.45771-1.69775-0.20000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 60s 906ms/step - loss: 1.4305 - categorical_accuracy: 0.4229 - val_loss: 1.9246 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2521_05_10.942506\\model-00003-1.43052-0.42289-1.92463-0.21000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 60s 904ms/step - loss: 1.2990 - categorical_accuracy: 0.5025 - val_loss: 2.2240 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2521_05_10.942506\\model-00004-1.29897-0.50249-2.22404-0.23000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 59s 892ms/step - loss: 1.1551 - categorical_accuracy: 0.5423 - val_loss: 1.8201 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2521_05_10.942506\\model-00005-1.15506-0.54229-1.82008-0.23000.h5\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 58s 881ms/step - loss: 1.2764 - categorical_accuracy: 0.5124 - val_loss: 2.0466 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2521_05_10.942506\\model-00006-1.27641-0.51244-2.04663-0.19000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 57s 869ms/step - loss: 1.0961 - categorical_accuracy: 0.5970 - val_loss: 2.5433 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2521_05_10.942506\\model-00007-1.09609-0.59701-2.54330-0.16000.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 60s 915ms/step - loss: 0.9691 - categorical_accuracy: 0.6070 - val_loss: 2.4632 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2521_05_10.942506\\model-00008-0.96914-0.60697-2.46320-0.28000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 59s 886ms/step - loss: 1.0142 - categorical_accuracy: 0.6318 - val_loss: 2.1983 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2521_05_10.942506\\model-00009-1.01416-0.63184-2.19825-0.27000.h5\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 57s 860ms/step - loss: 0.7381 - categorical_accuracy: 0.7562 - val_loss: 1.5737 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2521_05_10.942506\\model-00010-0.73810-0.75622-1.57374-0.44000.h5\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 61s 919ms/step - loss: 0.8605 - categorical_accuracy: 0.6915 - val_loss: 1.1134 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2521_05_10.942506\\model-00011-0.86054-0.69154-1.11339-0.59000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 58s 877ms/step - loss: 0.6974 - categorical_accuracy: 0.7463 - val_loss: 0.8537 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2521_05_10.942506\\model-00012-0.69742-0.74627-0.85374-0.71000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.6918 - categorical_accuracy: 0.7114 - val_loss: 0.9742 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2521_05_10.942506\\model-00013-0.69176-0.71144-0.97421-0.64000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 57s 856ms/step - loss: 0.6610 - categorical_accuracy: 0.7861 - val_loss: 0.9382 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2521_05_10.942506\\model-00014-0.66098-0.78607-0.93820-0.69000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 58s 875ms/step - loss: 0.5888 - categorical_accuracy: 0.7861 - val_loss: 0.9131 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2521_05_10.942506\\model-00015-0.58881-0.78607-0.91312-0.67000.h5\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 58s 875ms/step - loss: 0.6238 - categorical_accuracy: 0.7612 - val_loss: 0.8480 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2521_05_10.942506\\model-00016-0.62377-0.76119-0.84802-0.74000.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 57s 867ms/step - loss: 0.6178 - categorical_accuracy: 0.7711 - val_loss: 0.9241 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2521_05_10.942506\\model-00017-0.61778-0.77114-0.92408-0.65000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.6051 - categorical_accuracy: 0.7861 - val_loss: 0.7318 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2521_05_10.942506\\model-00018-0.60511-0.78607-0.73180-0.77000.h5\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 64s 962ms/step - loss: 0.5243 - categorical_accuracy: 0.8557 - val_loss: 1.0112 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2521_05_10.942506\\model-00019-0.52427-0.85572-1.01119-0.69000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 65s 986ms/step - loss: 0.4852 - categorical_accuracy: 0.8209 - val_loss: 0.8861 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2521_05_10.942506\\model-00020-0.48524-0.82090-0.88612-0.72000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 66s 1s/step - loss: 0.3437 - categorical_accuracy: 0.8905 - val_loss: 0.8359 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-10-2521_05_10.942506\\model-00021-0.34366-0.89055-0.83592-0.76000.h5\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 66s 995ms/step - loss: 0.3666 - categorical_accuracy: 0.8607 - val_loss: 0.8138 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-10-2521_05_10.942506\\model-00022-0.36662-0.86070-0.81377-0.74000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 64s 972ms/step - loss: 0.3641 - categorical_accuracy: 0.8756 - val_loss: 0.8479 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-10-2521_05_10.942506\\model-00023-0.36407-0.87562-0.84792-0.74000.h5\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 66s 1s/step - loss: 0.2526 - categorical_accuracy: 0.9254 - val_loss: 0.8929 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-10-2521_05_10.942506\\model-00024-0.25265-0.92537-0.89294-0.71000.h5\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.3348 - categorical_accuracy: 0.9005 - val_loss: 0.9003 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-10-2521_05_10.942506\\model-00025-0.33482-0.90050-0.90031-0.74000.h5\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.2818 - categorical_accuracy: 0.9154 - val_loss: 0.9227 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00026: saving model to model_init_2021-10-2521_05_10.942506\\model-00026-0.28178-0.91542-0.92269-0.73000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.2256 - categorical_accuracy: 0.9453 - val_loss: 0.9207 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00027: saving model to model_init_2021-10-2521_05_10.942506\\model-00027-0.22562-0.94527-0.92070-0.74000.h5\n",
      "Epoch 28/30\n",
      "67/67 [==============================] - 64s 965ms/step - loss: 0.2648 - categorical_accuracy: 0.9204 - val_loss: 1.1082 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00028: saving model to model_init_2021-10-2521_05_10.942506\\model-00028-0.26478-0.92040-1.10821-0.68000.h5\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a562f5d850>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the model overfits as training accuracy is 92.04% and validation accuracy is 68%.and the model stops learning at 28th epoch as validation loss is not reducing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-12 :Add dropouts,and set the learning rate to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(x,y,z,channels)\n",
    "model_d3= Sequential()\n",
    "\n",
    "model_d3.add(TimeDistributed(Conv2D(16, kernel_size=(3, 3),  padding='same'),input_shape=input_shape))\n",
    "model_d3.add(TimeDistributed(Activation('relu')))\n",
    "model_d3.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d3.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "\n",
    "model_d3.add(TimeDistributed(Conv2D(32, kernel_size=(3, 3), padding='same')))\n",
    "model_d3.add(TimeDistributed(Activation('relu')))\n",
    "model_d3.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d3.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "model_d3.add(TimeDistributed(Conv2D(64, kernel_size=(3, 3),  padding='same')))\n",
    "model_d3.add(TimeDistributed(Activation('relu')))\n",
    "model_d3.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d3.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "\n",
    "model_d3.add(TimeDistributed(Conv2D(128, kernel_size=(3, 3), padding='same')))\n",
    "model_d3.add(TimeDistributed(Activation('relu')))\n",
    "model_d3.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "\n",
    "model_d3.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_d3.add(Dropout(0.25))\n",
    "\n",
    "model_d3.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_d3.add(GRU(128,return_sequences=True))\n",
    "model_d3.add(Dropout(0.25))\n",
    "\n",
    "model_d3.add(GRU(128))\n",
    "model_d3.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model_d3.add(Dense(128,activation='relu'))\n",
    "model_d3.add(Dropout(0.25))\n",
    "\n",
    "model_d3.add(Dense(classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_241 (TimeDi (None, 30, 100, 100, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_242 (TimeDi (None, 30, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_243 (TimeDi (None, 30, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "time_distributed_244 (TimeDi (None, 30, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_245 (TimeDi (None, 30, 50, 50, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_246 (TimeDi (None, 30, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_247 (TimeDi (None, 30, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_248 (TimeDi (None, 30, 25, 25, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_249 (TimeDi (None, 30, 25, 25, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_250 (TimeDi (None, 30, 25, 25, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_251 (TimeDi (None, 30, 25, 25, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_252 (TimeDi (None, 30, 12, 12, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_253 (TimeDi (None, 30, 12, 12, 128)   73856     \n",
      "_________________________________________________________________\n",
      "time_distributed_254 (TimeDi (None, 30, 12, 12, 128)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_255 (TimeDi (None, 30, 12, 12, 128)   512       \n",
      "_________________________________________________________________\n",
      "time_distributed_256 (TimeDi (None, 30, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 30, 6, 6, 128)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_257 (TimeDi (None, 30, 4608)          0         \n",
      "_________________________________________________________________\n",
      "gru_30 (GRU)                 (None, 30, 128)           1819392   \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_31 (GRU)                 (None, 128)               99072     \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,034,021\n",
      "Trainable params: 2,033,541\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam(0.0001)\n",
    "model_d3.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_d3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 10\n",
      "Epoch 1/30\n",
      "65/67 [============================>.] - ETA: 3s - loss: 1.4927 - categorical_accuracy: 0.3523Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4909 - categorical_accuracy: 0.3544Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 10\n",
      "67/67 [==============================] - 155s 2s/step - loss: 1.4909 - categorical_accuracy: 0.3544 - val_loss: 1.6418 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2521_05_10.942506\\model-00001-1.49089-0.35445-1.64185-0.22000.h5\n",
      "Epoch 2/30\n",
      "67/67 [==============================] - 61s 919ms/step - loss: 1.2475 - categorical_accuracy: 0.4925 - val_loss: 1.6505 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2521_05_10.942506\\model-00002-1.24750-0.49254-1.65047-0.20000.h5\n",
      "Epoch 3/30\n",
      "67/67 [==============================] - 61s 924ms/step - loss: 1.2965 - categorical_accuracy: 0.4925 - val_loss: 1.7960 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2521_05_10.942506\\model-00003-1.29653-0.49254-1.79601-0.20000.h5\n",
      "Epoch 4/30\n",
      "67/67 [==============================] - 61s 928ms/step - loss: 1.0724 - categorical_accuracy: 0.5821 - val_loss: 2.1903 - val_categorical_accuracy: 0.1500\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2521_05_10.942506\\model-00004-1.07243-0.58209-2.19028-0.15000.h5\n",
      "Epoch 5/30\n",
      "67/67 [==============================] - 63s 948ms/step - loss: 0.9269 - categorical_accuracy: 0.6866 - val_loss: 2.2986 - val_categorical_accuracy: 0.1400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2521_05_10.942506\\model-00005-0.92690-0.68657-2.29860-0.14000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 6/30\n",
      "67/67 [==============================] - 61s 930ms/step - loss: 0.8853 - categorical_accuracy: 0.6567 - val_loss: 2.3783 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2521_05_10.942506\\model-00006-0.88527-0.65672-2.37832-0.17000.h5\n",
      "Epoch 7/30\n",
      "67/67 [==============================] - 59s 897ms/step - loss: 0.8052 - categorical_accuracy: 0.7264 - val_loss: 2.4713 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2521_05_10.942506\\model-00007-0.80525-0.72637-2.47133-0.16000.h5\n",
      "Epoch 8/30\n",
      "67/67 [==============================] - 63s 957ms/step - loss: 0.7432 - categorical_accuracy: 0.7413 - val_loss: 2.3451 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2521_05_10.942506\\model-00008-0.74324-0.74129-2.34507-0.17000.h5\n",
      "Epoch 9/30\n",
      "67/67 [==============================] - 58s 878ms/step - loss: 0.6972 - categorical_accuracy: 0.7413 - val_loss: 2.4474 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2521_05_10.942506\\model-00009-0.69724-0.74129-2.44744-0.21000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "Epoch 10/30\n",
      "67/67 [==============================] - 61s 916ms/step - loss: 0.7418 - categorical_accuracy: 0.7363 - val_loss: 1.8618 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2521_05_10.942506\\model-00010-0.74178-0.73632-1.86185-0.34000.h5\n",
      "Epoch 11/30\n",
      "67/67 [==============================] - 62s 940ms/step - loss: 0.6944 - categorical_accuracy: 0.7363 - val_loss: 1.3728 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2521_05_10.942506\\model-00011-0.69444-0.73632-1.37282-0.50000.h5\n",
      "Epoch 12/30\n",
      "67/67 [==============================] - 59s 899ms/step - loss: 0.6643 - categorical_accuracy: 0.7662 - val_loss: 0.9909 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2521_05_10.942506\\model-00012-0.66430-0.76617-0.99091-0.62000.h5\n",
      "Epoch 13/30\n",
      "67/67 [==============================] - 58s 879ms/step - loss: 0.6132 - categorical_accuracy: 0.8060 - val_loss: 0.8610 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2521_05_10.942506\\model-00013-0.61325-0.80597-0.86100-0.67000.h5\n",
      "Epoch 14/30\n",
      "67/67 [==============================] - 63s 956ms/step - loss: 0.6279 - categorical_accuracy: 0.8060 - val_loss: 0.8022 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2521_05_10.942506\\model-00014-0.62786-0.80597-0.80224-0.64000.h5\n",
      "Epoch 15/30\n",
      "67/67 [==============================] - 58s 885ms/step - loss: 0.7983 - categorical_accuracy: 0.7015 - val_loss: 0.7527 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2521_05_10.942506\\model-00015-0.79826-0.70149-0.75271-0.72000.h5\n",
      "Epoch 16/30\n",
      "67/67 [==============================] - 59s 888ms/step - loss: 0.6824 - categorical_accuracy: 0.7512 - val_loss: 0.7140 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2521_05_10.942506\\model-00016-0.68236-0.75124-0.71397-0.73000.h5\n",
      "Epoch 17/30\n",
      "67/67 [==============================] - 60s 910ms/step - loss: 0.5767 - categorical_accuracy: 0.8209 - val_loss: 0.7751 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2521_05_10.942506\\model-00017-0.57666-0.82090-0.77509-0.66000.h5\n",
      "Epoch 18/30\n",
      "67/67 [==============================] - 59s 889ms/step - loss: 0.6277 - categorical_accuracy: 0.8060 - val_loss: 0.6481 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2521_05_10.942506\\model-00018-0.62772-0.80597-0.64807-0.75000.h5\n",
      "Epoch 19/30\n",
      "67/67 [==============================] - 59s 888ms/step - loss: 0.6544 - categorical_accuracy: 0.7612 - val_loss: 0.8387 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2521_05_10.942506\\model-00019-0.65440-0.76119-0.83870-0.65000.h5\n",
      "Epoch 20/30\n",
      "67/67 [==============================] - 59s 897ms/step - loss: 0.6626 - categorical_accuracy: 0.8060 - val_loss: 0.7333 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2521_05_10.942506\\model-00020-0.66260-0.80597-0.73335-0.68000.h5\n",
      "Epoch 21/30\n",
      "67/67 [==============================] - 61s 925ms/step - loss: 0.6188 - categorical_accuracy: 0.7711 - val_loss: 0.7509 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-10-2521_05_10.942506\\model-00021-0.61882-0.77114-0.75086-0.69000.h5\n",
      "Epoch 22/30\n",
      "67/67 [==============================] - 59s 899ms/step - loss: 0.6271 - categorical_accuracy: 0.8010 - val_loss: 0.7433 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-10-2521_05_10.942506\\model-00022-0.62713-0.80100-0.74327-0.71000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "Epoch 23/30\n",
      "67/67 [==============================] - 58s 878ms/step - loss: 0.6457 - categorical_accuracy: 0.7861 - val_loss: 0.7359 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-10-2521_05_10.942506\\model-00023-0.64573-0.78607-0.73587-0.70000.h5\n",
      "Epoch 24/30\n",
      "67/67 [==============================] - 63s 951ms/step - loss: 0.5979 - categorical_accuracy: 0.8159 - val_loss: 0.7295 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-10-2521_05_10.942506\\model-00024-0.59786-0.81592-0.72945-0.68000.h5\n",
      "Epoch 25/30\n",
      "67/67 [==============================] - 58s 884ms/step - loss: 0.5795 - categorical_accuracy: 0.8259 - val_loss: 0.7685 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-10-2521_05_10.942506\\model-00025-0.57949-0.82587-0.76852-0.74000.h5\n",
      "Epoch 26/30\n",
      "67/67 [==============================] - 59s 899ms/step - loss: 0.6412 - categorical_accuracy: 0.7811 - val_loss: 0.8533 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00026: saving model to model_init_2021-10-2521_05_10.942506\\model-00026-0.64119-0.78109-0.85333-0.63000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "Epoch 27/30\n",
      "67/67 [==============================] - 61s 919ms/step - loss: 0.6162 - categorical_accuracy: 0.8010 - val_loss: 0.6961 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00027: saving model to model_init_2021-10-2521_05_10.942506\\model-00027-0.61622-0.80100-0.69609-0.75000.h5\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 57s 862ms/step - loss: 0.5793 - categorical_accuracy: 0.8507 - val_loss: 0.7219 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00028: saving model to model_init_2021-10-2521_05_10.942506\\model-00028-0.57930-0.85075-0.72186-0.70000.h5\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a54c2b4640>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results we saw that adding dropouts and reducing the learning rate we decrease the training accuracy to 85.05% and the validation accuracy is increased to 70.0%.<br> But the model still overfits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-13 : lets build a model from a pretrained architecture by using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 30 # number of frames\n",
    "y = 100 # image width\n",
    "z = 100 # image height\n",
    "batch_size=5\n",
    "num_epochs=30\n",
    "channels=3\n",
    "classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx =[x for x in range(0,x)]   #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size    # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            batch_data = np.zeros((batch_size,x,y,z,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,classes)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    temp = imresize(image,(y,z))\n",
    "                    temp = temp/255 #Normalize data\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = \"C:\\cnndatasets\\Project_data/train\"\n",
    "val_path = \"C:\\cnndatasets\\Project_data/val\"\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs =   30  # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_batches = num_train_sequences//batch_size \n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import mobilenet\n",
    "mobile_net = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in mobile_net.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_e = Sequential()\n",
    "model_e.add(TimeDistributed(mobile_net,input_shape=(x,y,z,channels)))\n",
    "\n",
    "\n",
    "\n",
    "model_e.add(TimeDistributed(BatchNormalization()))\n",
    "model_e.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_e.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_e.add(GRU(128))\n",
    "model_e.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_e.add(Dense(128,activation='relu'))\n",
    "model_e.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_e.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_73 (TimeDis (None, 30, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_74 (TimeDis (None, 30, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_75 (TimeDis (None, 30, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_76 (TimeDis (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru_22 (GRU)                 (None, 128)               443136    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 462,341\n",
      "Non-trainable params: 3,230,912\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_e.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_e.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 5\n",
      "Epoch 1/30\n",
      "131/133 [============================>.] - ETA: 2s - loss: 1.6463 - categorical_accuracy: 0.2214Batch:  133 Index: 5\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.6418 - categorical_accuracy: 0.2247Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 5\n",
      "133/133 [==============================] - 163s 1s/step - loss: 1.6418 - categorical_accuracy: 0.2247 - val_loss: 1.5816 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2712_52_55.420864\\model-00001-1.64179-0.22474-1.58165-0.27000.h5\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 104s 786ms/step - loss: 1.5994 - categorical_accuracy: 0.2707 - val_loss: 1.6248 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2712_52_55.420864\\model-00002-1.59939-0.27068-1.62482-0.24000.h5\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 97s 731ms/step - loss: 1.5995 - categorical_accuracy: 0.3133 - val_loss: 1.5573 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2712_52_55.420864\\model-00003-1.59950-0.31328-1.55735-0.27000.h5\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 103s 782ms/step - loss: 1.5599 - categorical_accuracy: 0.2857 - val_loss: 1.5215 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2712_52_55.420864\\model-00004-1.55993-0.28571-1.52149-0.32000.h5\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 96s 728ms/step - loss: 1.5228 - categorical_accuracy: 0.3083 - val_loss: 1.4585 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2712_52_55.420864\\model-00005-1.52281-0.30827-1.45850-0.32000.h5\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 102s 773ms/step - loss: 1.5357 - categorical_accuracy: 0.2757 - val_loss: 1.4934 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2712_52_55.420864\\model-00006-1.53572-0.27569-1.49337-0.29000.h5\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 98s 745ms/step - loss: 1.5157 - categorical_accuracy: 0.3233 - val_loss: 1.4740 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2712_52_55.420864\\model-00007-1.51569-0.32331-1.47405-0.39000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 99s 747ms/step - loss: 1.4834 - categorical_accuracy: 0.3283 - val_loss: 1.4621 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2712_52_55.420864\\model-00008-1.48336-0.32832-1.46211-0.32000.h5\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 105s 797ms/step - loss: 1.4396 - categorical_accuracy: 0.3409 - val_loss: 1.4350 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2712_52_55.420864\\model-00009-1.43961-0.34085-1.43498-0.34000.h5\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 101s 764ms/step - loss: 1.4065 - categorical_accuracy: 0.4010 - val_loss: 1.4214 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2712_52_55.420864\\model-00010-1.40649-0.40100-1.42142-0.39000.h5\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 104s 789ms/step - loss: 1.4121 - categorical_accuracy: 0.3734 - val_loss: 1.4666 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2712_52_55.420864\\model-00011-1.41212-0.37343-1.46661-0.30000.h5\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 101s 761ms/step - loss: 1.4253 - categorical_accuracy: 0.3609 - val_loss: 1.4679 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2712_52_55.420864\\model-00012-1.42526-0.36090-1.46791-0.33000.h5\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 99s 746ms/step - loss: 1.3755 - categorical_accuracy: 0.3860 - val_loss: 1.4681 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2712_52_55.420864\\model-00013-1.37548-0.38596-1.46808-0.39000.h5\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 102s 776ms/step - loss: 1.3396 - categorical_accuracy: 0.4135 - val_loss: 1.3902 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2712_52_55.420864\\model-00014-1.33962-0.41353-1.39016-0.41000.h5\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 96s 724ms/step - loss: 1.3715 - categorical_accuracy: 0.4110 - val_loss: 1.3874 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2712_52_55.420864\\model-00015-1.37146-0.41103-1.38736-0.36000.h5\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 104s 789ms/step - loss: 1.3579 - categorical_accuracy: 0.4135 - val_loss: 1.4578 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2712_52_55.420864\\model-00016-1.35791-0.41353-1.45779-0.39000.h5\n",
      "Epoch 17/30\n",
      "133/133 [==============================] - 97s 736ms/step - loss: 1.3671 - categorical_accuracy: 0.3960 - val_loss: 1.3393 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2021-10-2712_52_55.420864\\model-00017-1.36713-0.39599-1.33934-0.42000.h5\n",
      "Epoch 18/30\n",
      "133/133 [==============================] - 405s 3s/step - loss: 1.2944 - categorical_accuracy: 0.4160 - val_loss: 1.4153 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00018: saving model to model_init_2021-10-2712_52_55.420864\\model-00018-1.29436-0.41604-1.41530-0.41000.h5\n",
      "Epoch 19/30\n",
      "133/133 [==============================] - 120s 905ms/step - loss: 1.3026 - categorical_accuracy: 0.4211 - val_loss: 1.4524 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00019: saving model to model_init_2021-10-2712_52_55.420864\\model-00019-1.30261-0.42105-1.45243-0.37000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 20/30\n",
      "133/133 [==============================] - 115s 873ms/step - loss: 1.2250 - categorical_accuracy: 0.4536 - val_loss: 1.4184 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00020: saving model to model_init_2021-10-2712_52_55.420864\\model-00020-1.22495-0.45363-1.41837-0.42000.h5\n",
      "Epoch 21/30\n",
      "133/133 [==============================] - 96s 724ms/step - loss: 1.2523 - categorical_accuracy: 0.4411 - val_loss: 1.3974 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00021: saving model to model_init_2021-10-2712_52_55.420864\\model-00021-1.25235-0.44110-1.39741-0.44000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 22/30\n",
      "133/133 [==============================] - 93s 704ms/step - loss: 1.2338 - categorical_accuracy: 0.5063 - val_loss: 1.3663 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00022: saving model to model_init_2021-10-2712_52_55.420864\\model-00022-1.23384-0.50627-1.36629-0.45000.h5\n",
      "Epoch 23/30\n",
      "133/133 [==============================] - 90s 682ms/step - loss: 1.2336 - categorical_accuracy: 0.4687 - val_loss: 1.4756 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2021-10-2712_52_55.420864\\model-00023-1.23364-0.46867-1.47555-0.40000.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 24/30\n",
      "133/133 [==============================] - 95s 717ms/step - loss: 1.2183 - categorical_accuracy: 0.4662 - val_loss: 1.3574 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00024: saving model to model_init_2021-10-2712_52_55.420864\\model-00024-1.21828-0.46617-1.35743-0.49000.h5\n",
      "Epoch 25/30\n",
      "133/133 [==============================] - 89s 676ms/step - loss: 1.2559 - categorical_accuracy: 0.4812 - val_loss: 1.4480 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00025: saving model to model_init_2021-10-2712_52_55.420864\\model-00025-1.25591-0.48120-1.44800-0.48000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 26/30\n",
      "133/133 [==============================] - 93s 703ms/step - loss: 1.1958 - categorical_accuracy: 0.5263 - val_loss: 1.3558 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00026: saving model to model_init_2021-10-2712_52_55.420864\\model-00026-1.19581-0.52632-1.35580-0.49000.h5\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 93s 706ms/step - loss: 1.2493 - categorical_accuracy: 0.4612 - val_loss: 1.3985 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00027: saving model to model_init_2021-10-2712_52_55.420864\\model-00027-1.24925-0.46115-1.39854-0.42000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fc295de220>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_e.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying numerous times with resnet and mobilenet pretrained  models,we saw that the training loss and validation loss was not decreasing and hence the model stops learning for batch size=10.<br>So we kept batch size=5 and we got training accuracy of 46.12% and validation accuracy of 42%.<br>The model is generalisable but the acuracies are low ,so to improve the accuracies we train some layers of mobilenet parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import mobilenet\n",
    "mobile_net = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in mobile_net.layers[:-10]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_e1 = Sequential()\n",
    "model_e1.add(TimeDistributed(mobile_net,input_shape=(x,y,z,channels)))\n",
    "\n",
    "\n",
    "\n",
    "model_e1.add(TimeDistributed(BatchNormalization()))\n",
    "model_e1.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model_e1.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model_e1.add(GRU(128))\n",
    "model_e1.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_e1.add(Dense(128,activation='relu'))\n",
    "model_e1.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_e1.add(Dense(classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_85 (TimeDis (None, 30, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_86 (TimeDis (None, 30, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_87 (TimeDis (None, 30, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_88 (TimeDis (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru_25 (GRU)                 (None, 128)               443136    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 2,050,565\n",
      "Non-trainable params: 1,642,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser=tf.keras.optimizers.Adam()\n",
    "model_e1.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model_e1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
    "earlystop = EarlyStopping( monitor=\"val_loss\", min_delta=0,patience=10,verbose=1)\n",
    "callbacks_list = [checkpoint, LR, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\cnndatasets\\Project_data/train ; batch size = 5\n",
      "Epoch 1/30\n",
      "131/133 [============================>.] - ETA: 1s - loss: 1.8220 - categorical_accuracy: 0.1863Batch:  133 Index: 5\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.8199 - categorical_accuracy: 0.1870Source path =  C:\\cnndatasets\\Project_data/val ; batch size = 5\n",
      "133/133 [==============================] - 145s 1s/step - loss: 1.8199 - categorical_accuracy: 0.1870 - val_loss: 1.6092 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-10-2712_52_55.420864\\model-00001-1.81990-0.18703-1.60919-0.23000.h5\n",
      "Epoch 2/30\n",
      "133/133 [==============================] - 92s 693ms/step - loss: 1.7065 - categorical_accuracy: 0.1955 - val_loss: 1.6230 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2021-10-2712_52_55.420864\\model-00002-1.70652-0.19549-1.62296-0.16000.h5\n",
      "Epoch 3/30\n",
      "133/133 [==============================] - 95s 718ms/step - loss: 1.6470 - categorical_accuracy: 0.2406 - val_loss: 1.6229 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2021-10-2712_52_55.420864\\model-00003-1.64702-0.24060-1.62290-0.25000.h5\n",
      "Epoch 4/30\n",
      "133/133 [==============================] - 96s 728ms/step - loss: 1.6369 - categorical_accuracy: 0.2005 - val_loss: 1.6145 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2021-10-2712_52_55.420864\\model-00004-1.63691-0.20050-1.61448-0.23000.h5\n",
      "Epoch 5/30\n",
      "133/133 [==============================] - 95s 718ms/step - loss: 1.6207 - categorical_accuracy: 0.2130 - val_loss: 1.6118 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2021-10-2712_52_55.420864\\model-00005-1.62070-0.21303-1.61181-0.23000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/30\n",
      "133/133 [==============================] - 93s 708ms/step - loss: 1.6218 - categorical_accuracy: 0.1830 - val_loss: 1.6039 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2021-10-2712_52_55.420864\\model-00006-1.62177-0.18296-1.60394-0.21000.h5\n",
      "Epoch 7/30\n",
      "133/133 [==============================] - 92s 696ms/step - loss: 1.6168 - categorical_accuracy: 0.1779 - val_loss: 1.6097 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2021-10-2712_52_55.420864\\model-00007-1.61684-0.17794-1.60971-0.21000.h5\n",
      "Epoch 8/30\n",
      "133/133 [==============================] - 95s 719ms/step - loss: 1.6102 - categorical_accuracy: 0.2055 - val_loss: 1.6099 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00008: saving model to model_init_2021-10-2712_52_55.420864\\model-00008-1.61017-0.20551-1.60988-0.17000.h5\n",
      "Epoch 9/30\n",
      "133/133 [==============================] - 95s 718ms/step - loss: 1.6176 - categorical_accuracy: 0.2005 - val_loss: 1.6071 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00009: saving model to model_init_2021-10-2712_52_55.420864\\model-00009-1.61763-0.20050-1.60706-0.19000.h5\n",
      "Epoch 10/30\n",
      "133/133 [==============================] - 92s 698ms/step - loss: 1.6096 - categorical_accuracy: 0.1880 - val_loss: 1.6062 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2021-10-2712_52_55.420864\\model-00010-1.60956-0.18797-1.60616-0.19000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 11/30\n",
      "133/133 [==============================] - 93s 704ms/step - loss: 1.6112 - categorical_accuracy: 0.1980 - val_loss: 1.6098 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2021-10-2712_52_55.420864\\model-00011-1.61119-0.19799-1.60980-0.16000.h5\n",
      "Epoch 12/30\n",
      "133/133 [==============================] - 92s 693ms/step - loss: 1.6104 - categorical_accuracy: 0.2331 - val_loss: 1.6101 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00012: saving model to model_init_2021-10-2712_52_55.420864\\model-00012-1.61042-0.23308-1.61014-0.17000.h5\n",
      "Epoch 13/30\n",
      "133/133 [==============================] - 94s 710ms/step - loss: 1.6115 - categorical_accuracy: 0.2130 - val_loss: 1.6076 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00013: saving model to model_init_2021-10-2712_52_55.420864\\model-00013-1.61151-0.21303-1.60762-0.18000.h5\n",
      "Epoch 14/30\n",
      "133/133 [==============================] - 94s 711ms/step - loss: 1.6085 - categorical_accuracy: 0.2206 - val_loss: 1.6093 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2021-10-2712_52_55.420864\\model-00014-1.60850-0.22055-1.60929-0.19000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/30\n",
      "133/133 [==============================] - 93s 707ms/step - loss: 1.6086 - categorical_accuracy: 0.2030 - val_loss: 1.6102 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00015: saving model to model_init_2021-10-2712_52_55.420864\\model-00015-1.60863-0.20301-1.61019-0.16000.h5\n",
      "Epoch 16/30\n",
      "133/133 [==============================] - 92s 697ms/step - loss: 1.6085 - categorical_accuracy: 0.1855 - val_loss: 1.6093 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00016: saving model to model_init_2021-10-2712_52_55.420864\\model-00016-1.60848-0.18546-1.60926-0.19000.h5\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fc3b067fd0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_e1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see even the trainable parameters increased the loss is not all decreasing.hence the model stops learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thus final model is Exp-6 with Training Accuracy: 0.94, Validation accuracy:-0.88 with least number of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
